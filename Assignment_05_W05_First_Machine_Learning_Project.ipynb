{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-05-W05-First-Machine-Learning-Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjfxf/in6Fa43FSF3ona7V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobars/Assignment-05-W05-First-Machine-Learning-Project/blob/master/Assignment_05_W05_First_Machine_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90n3Bdp_Y7Tc",
        "colab_type": "text"
      },
      "source": [
        "In this colabs, we try to do a binary classification to determine whether a video published on YouTube will reach a certain number of views. The dataset is taken from Kaggle which can be seen from the following link https://www.kaggle.com/datasnaek/youtube-new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9DM4bllaemD",
        "colab_type": "text"
      },
      "source": [
        "# Use the right version of TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WST6-qafauoR",
        "colab_type": "text"
      },
      "source": [
        "The following hidden code cell ensures that the Colab will run on TensorFlow 2.X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiGXiy1Z26Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hslssLSaa-_Y",
        "colab_type": "text"
      },
      "source": [
        "# Call the import statements\n",
        "\n",
        "The following code imports the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDpRd_3obSuC",
        "colab_type": "code",
        "outputId": "81ddb1e2-b01e-42d0-dbae-66282634a57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Load the imports\n",
        "\n",
        "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import string,math,pickle\n",
        "from textblob import TextBlob\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "# tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "print(\"Ran the import statements.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ran the import statements.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9J747-RsyE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/kobars/Assignment-05-W05-First-Machine-Learning-Project/master/Ind-daily-updated-data/merged.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wUbgeAvs6FP",
        "colab_type": "code",
        "outputId": "cfe2a491-a497-4eeb-ed50-08aba8d8d0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>title</th>\n",
              "      <th>publishedAt</th>\n",
              "      <th>channelId</th>\n",
              "      <th>channelTitle</th>\n",
              "      <th>categoryId</th>\n",
              "      <th>trending_date</th>\n",
              "      <th>tags</th>\n",
              "      <th>view_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>dislikes</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>thumbnail_link</th>\n",
              "      <th>comments_disabled</th>\n",
              "      <th>ratings_disabled</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MF1queeLUU8</td>\n",
              "      <td>AISYAH ISTRI RASULULLAH - SYAKIR DAULAY  [ COV...</td>\n",
              "      <td>2020-03-29T10:00:05.000Z</td>\n",
              "      <td>UC2zvYA-I_HtKfQnjNGY2VFA</td>\n",
              "      <td>Syakir Daulay</td>\n",
              "      <td>10</td>\n",
              "      <td>20.01.04</td>\n",
              "      <td>di rumah aja|aisyah istri rasulullah|aisyah|sy...</td>\n",
              "      <td>6103246</td>\n",
              "      <td>408534</td>\n",
              "      <td>4498</td>\n",
              "      <td>27080</td>\n",
              "      <td>https://i.ytimg.com/vi/MF1queeLUU8/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Assalamualaikum wr. wb.Alhamdulillah hari ini ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aN0ZnoRg_IY</td>\n",
              "      <td>SABYAN - AISYAH ISTRI RASULULLAH | COVER</td>\n",
              "      <td>2020-03-28T02:10:45.000Z</td>\n",
              "      <td>UCeZA77FZSG4YzBcvBJ9eynA</td>\n",
              "      <td>SABYAN</td>\n",
              "      <td>10</td>\n",
              "      <td>20.01.04</td>\n",
              "      <td>Nissa sabyan|sabyan gambus|sabyan aisyah|ya al...</td>\n",
              "      <td>7830616</td>\n",
              "      <td>553305</td>\n",
              "      <td>7770</td>\n",
              "      <td>57050</td>\n",
              "      <td>https://i.ytimg.com/vi/aN0ZnoRg_IY/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Assalamualaikum wr. wb.Alhamdulillah hari ini ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ZphLn5AJH2Y</td>\n",
              "      <td>Amerika Serikat Kini Jadi Episentrum Corona</td>\n",
              "      <td>2020-03-30T05:30:09.000Z</td>\n",
              "      <td>UCGN9JsnkvK05v2lnTI_-uGA</td>\n",
              "      <td>CNBC Indonesia</td>\n",
              "      <td>22</td>\n",
              "      <td>20.01.04</td>\n",
              "      <td>cnbc indonesia|ekonomi|bisnis|indonesia|corona...</td>\n",
              "      <td>1208631</td>\n",
              "      <td>8065</td>\n",
              "      <td>776</td>\n",
              "      <td>2963</td>\n",
              "      <td>https://i.ytimg.com/vi/ZphLn5AJH2Y/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Kasus positif virus Corona di Amerika Serikat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ST28wZWevMI</td>\n",
              "      <td>AISYAH ISTRI RASULULLAH - COVEER BY TRI SUAKA</td>\n",
              "      <td>2020-03-30T12:00:22.000Z</td>\n",
              "      <td>UCUE-BF1Q5E6Sz1KidSnSRYw</td>\n",
              "      <td>musisi jogja project</td>\n",
              "      <td>10</td>\n",
              "      <td>20.01.04</td>\n",
              "      <td>Cover musik|musisi jalanan|musisi jogja|pengam...</td>\n",
              "      <td>1459470</td>\n",
              "      <td>73587</td>\n",
              "      <td>1269</td>\n",
              "      <td>3602</td>\n",
              "      <td>https://i.ytimg.com/vi/ST28wZWevMI/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>AISYAH ISTRI RASULULLAHOriginal Song by Projec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lTnjVMV9jmY</td>\n",
              "      <td>KENAPA CUMA LOE YG BERANI NGOMONG?! JAKARTA SU...</td>\n",
              "      <td>2020-03-28T03:57:47.000Z</td>\n",
              "      <td>UCYk4LJI0Pr6RBDWowMm-KUw</td>\n",
              "      <td>Deddy Corbuzier</td>\n",
              "      <td>24</td>\n",
              "      <td>20.01.04</td>\n",
              "      <td>anies baswedan|jokowi|Covid19|Indonesia|jakart...</td>\n",
              "      <td>4953538</td>\n",
              "      <td>223743</td>\n",
              "      <td>10107</td>\n",
              "      <td>56781</td>\n",
              "      <td>https://i.ytimg.com/vi/lTnjVMV9jmY/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>#indonesia #corona #covid19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3402</th>\n",
              "      <td>7dye45y2g8I</td>\n",
              "      <td>AISYAH ISTERI ROSULULLOH | VEVE ZULFIKAR BASYA...</td>\n",
              "      <td>2020-03-26T11:07:37.000Z</td>\n",
              "      <td>UCKxHsgyk3VLq_5DL63RX2zQ</td>\n",
              "      <td>VEVE ZULFIKAR</td>\n",
              "      <td>10</td>\n",
              "      <td>20.31.03</td>\n",
              "      <td>[none]</td>\n",
              "      <td>194236</td>\n",
              "      <td>11250</td>\n",
              "      <td>147</td>\n",
              "      <td>504</td>\n",
              "      <td>https://i.ytimg.com/vi/7dye45y2g8I/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>terima kasih banyak buat temen2 yg usul cover ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3403</th>\n",
              "      <td>X9s_PXxT2zg</td>\n",
              "      <td>Sedang Sayang Sayangnya - Mawar De Jongh | Cov...</td>\n",
              "      <td>2020-03-24T10:00:07.000Z</td>\n",
              "      <td>UC9hx5-f_s3L-mw_7ORsVrLg</td>\n",
              "      <td>Billy Joe Ava</td>\n",
              "      <td>10</td>\n",
              "      <td>20.31.03</td>\n",
              "      <td>billy joe ava|billy|cover lagu|cover|cover bil...</td>\n",
              "      <td>196234</td>\n",
              "      <td>11826</td>\n",
              "      <td>56</td>\n",
              "      <td>772</td>\n",
              "      <td>https://i.ytimg.com/vi/X9s_PXxT2zg/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Sedang Sayang Sayangnya - Mawar De Jongh Cover...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404</th>\n",
              "      <td>vx8BZpK5l0s</td>\n",
              "      <td>TEMBAK PANTAT NYA!!   (RADITYA DIKA DAN BAYI M...</td>\n",
              "      <td>2020-03-27T04:54:32.000Z</td>\n",
              "      <td>UCYk4LJI0Pr6RBDWowMm-KUw</td>\n",
              "      <td>Deddy Corbuzier</td>\n",
              "      <td>24</td>\n",
              "      <td>20.31.03</td>\n",
              "      <td>raditya dika|gabut|bayi manusia|stand up|lucu|...</td>\n",
              "      <td>1521733</td>\n",
              "      <td>62635</td>\n",
              "      <td>787</td>\n",
              "      <td>6442</td>\n",
              "      <td>https://i.ytimg.com/vi/vx8BZpK5l0s/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>#radityadika #santuy #gabut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3405</th>\n",
              "      <td>JX_Q8h4duaM</td>\n",
              "      <td>Air Suspension Drifting di Rumah! | Physical D...</td>\n",
              "      <td>2020-03-27T09:00:08.000Z</td>\n",
              "      <td>UCmA-OmY_ZW7VZkET-64Psdg</td>\n",
              "      <td>Garasi Drift</td>\n",
              "      <td>2</td>\n",
              "      <td>20.31.03</td>\n",
              "      <td>[none]</td>\n",
              "      <td>413096</td>\n",
              "      <td>18671</td>\n",
              "      <td>293</td>\n",
              "      <td>1376</td>\n",
              "      <td>https://i.ytimg.com/vi/JX_Q8h4duaM/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>English Subtitle Available---------\\rGarasi Dr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3406</th>\n",
              "      <td>RXfQHPiboTo</td>\n",
              "      <td>KARNA AKU MISKIN |Di Hina ! Cintaku Di Tolak I...</td>\n",
              "      <td>2020-03-21T09:51:07.000Z</td>\n",
              "      <td>UCTpgXXfJnRxLgZrEFu0J7PQ</td>\n",
              "      <td>ABE PROJECT</td>\n",
              "      <td>24</td>\n",
              "      <td>20.31.03</td>\n",
              "      <td>Video Baper|Aldo kembar|Corona|Dalan liyane|AB...</td>\n",
              "      <td>516782</td>\n",
              "      <td>5131</td>\n",
              "      <td>320</td>\n",
              "      <td>978</td>\n",
              "      <td>https://i.ytimg.com/vi/RXfQHPiboTo/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Cinta yang sesungguhnya Bukan dilihat dari har...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3407 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         video_id  ...                                        description\n",
              "0     MF1queeLUU8  ...  Assalamualaikum wr. wb.Alhamdulillah hari ini ...\n",
              "1     aN0ZnoRg_IY  ...  Assalamualaikum wr. wb.Alhamdulillah hari ini ...\n",
              "2     ZphLn5AJH2Y  ...  Kasus positif virus Corona di Amerika Serikat ...\n",
              "3     ST28wZWevMI  ...  AISYAH ISTRI RASULULLAHOriginal Song by Projec...\n",
              "4     lTnjVMV9jmY  ...                        #indonesia #corona #covid19\n",
              "...           ...  ...                                                ...\n",
              "3402  7dye45y2g8I  ...  terima kasih banyak buat temen2 yg usul cover ...\n",
              "3403  X9s_PXxT2zg  ...  Sedang Sayang Sayangnya - Mawar De Jongh Cover...\n",
              "3404  vx8BZpK5l0s  ...                        #radityadika #santuy #gabut\n",
              "3405  JX_Q8h4duaM  ...  English Subtitle Available---------\\rGarasi Dr...\n",
              "3406  RXfQHPiboTo  ...  Cinta yang sesungguhnya Bukan dilihat dari har...\n",
              "\n",
              "[3407 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ini2CEG5uJx",
        "colab_type": "code",
        "outputId": "5c45594a-5bdc-4abc-f678-573c2db97cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# is there a missing value?\n",
        "df.isna().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "video_id              0\n",
              "title                 0\n",
              "publishedAt           0\n",
              "channelId             0\n",
              "channelTitle          0\n",
              "                     ..\n",
              "comment_count         0\n",
              "thumbnail_link        0\n",
              "comments_disabled     0\n",
              "ratings_disabled      0\n",
              "description          76\n",
              "Length: 16, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFAWdWig53iB",
        "colab_type": "code",
        "outputId": "a9e184c4-5d6c-4efd-d117-fa430373b495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Displaying column info\n",
        "df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3407 entries, 0 to 3406\n",
            "Data columns (total 16 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   video_id           3407 non-null   object\n",
            " 1   title              3407 non-null   object\n",
            " 2   publishedAt        3407 non-null   object\n",
            " 3   channelId          3407 non-null   object\n",
            " 4   channelTitle       3407 non-null   object\n",
            " 5   categoryId         3407 non-null   int64 \n",
            " 6   trending_date      3407 non-null   object\n",
            " 7   tags               3407 non-null   object\n",
            " 8   view_count         3407 non-null   int64 \n",
            " 9   likes              3407 non-null   int64 \n",
            " 10  dislikes           3407 non-null   int64 \n",
            " 11  comment_count      3407 non-null   int64 \n",
            " 12  thumbnail_link     3407 non-null   object\n",
            " 13  comments_disabled  3407 non-null   bool  \n",
            " 14  ratings_disabled   3407 non-null   bool  \n",
            " 15  description        3331 non-null   object\n",
            "dtypes: bool(2), int64(5), object(9)\n",
            "memory usage: 379.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0VKXbm51eRP",
        "colab_type": "code",
        "outputId": "57299ef3-d862-45d4-ac29-be1e6460be2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3407, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LGW62aU4QgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['trending_date'] = pd.to_datetime(df['trending_date'],format='%y.%d.%m')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY4XM7b44YAO",
        "colab_type": "code",
        "outputId": "8cdb43e9-fa2a-4a54-9b85-2373a3961a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df['trending_date'].sort_values"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Series.sort_values of 0      2020-04-01\n",
              "1      2020-04-01\n",
              "2      2020-04-01\n",
              "3      2020-04-01\n",
              "4      2020-04-01\n",
              "          ...    \n",
              "3402   2020-03-31\n",
              "3403   2020-03-31\n",
              "3404   2020-03-31\n",
              "3405   2020-03-31\n",
              "3406   2020-03-31\n",
              "Name: trending_date, Length: 3407, dtype: datetime64[ns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKuESWwE7WsR",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3I_126b7uda",
        "colab_type": "text"
      },
      "source": [
        "The dataset does not have labels for how long a video stays in the trending list. Therefore, the data is reformatted for the purpose of this project before preforming data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_tkQL_b7wuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation (doc):\n",
        "    return \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in doc]).split())\n",
        "\n",
        "def tfidf_fun(word, blob, bloblist):\n",
        "    tf=blob.words.count(word) / len(blob.words)\n",
        "    n_containing=sum(1 for blob2 in bloblist if word in blob2.words)\n",
        "    idf=math.log(len(bloblist) / (1 + n_containing))\n",
        "    return tf * idf\n",
        "\n",
        "def avg_tfidf_fun(blob, bloblist):\n",
        "    avg_tfidf=0\n",
        "    for wordI in blob.words:\n",
        "        avg_tfidf=avg_tfidf+tfidf_fun(wordI, blob, bloblist)\n",
        "    if blob.words.__len__() != 0:\n",
        "        avg_tfidf=avg_tfidf/blob.words.__len__()\n",
        "        return avg_tfidf\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_RBM_nM7d33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numFeatures = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehC8QCDO7aTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the features and labels dataframe\n",
        "df_features = pd.DataFrame(np.zeros([df.shape[0],numFeatures]),columns=['days_was_trending', 'title_polarity', 'title_subjectivity','title_tfidf' ,\n",
        "                                                                             'tags_polarity', 'tags_subjectivity' , 'tags_tfidf','description_polarity',\n",
        "                                                                             'description_subjectivity', 'description_tfidf',\n",
        "                                                                             'categoryId','days_since_uploaded','view_count',\n",
        "                                                                             'likes','dislikes','comment_count','like_rate','dislike_rate',\n",
        "                                                                             'comment_rate','view_change','like_change','dislike_change','comment_change',\n",
        "                                                                             'comments_disabled','ratings_disabled'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjDD6_UR7_zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_label = pd.DataFrame(np.zeros([df.shape[0],2]),columns=['video_ind','days_will_be_trending'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xXeJmdN8wDC",
        "colab_type": "code",
        "outputId": "165434ed-ddfe-4104-f7d2-d458554731e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV4GkBYv8ILE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_ids=df['video_id'].unique()\n",
        "bloblist=[]\n",
        "for docI in range(unique_ids.shape[0]):\n",
        "    vedI_data = df[df['video_id'] == unique_ids[docI]]\n",
        "    vedI_data=vedI_data.reset_index(drop=True)\n",
        "    document = vedI_data.title[0]+ \" \" +vedI_data.tags[0]+\" \" +str(vedI_data.description[0])\n",
        "    bloblist.append(TextBlob(remove_punctuation(document)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S4cS4ue8Zbd",
        "colab_type": "code",
        "outputId": "a6160081-7116-47b1-f071-a312ab77a07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "count1=0\n",
        "for vidI in range(100):#range(unique_ids.shape[0]): #TODO remove the comment to use all the videos and not just 10\n",
        "    print('Extracting the features of video #:' + str(vidI))\n",
        "    vedI_data=df[df['video_id']==unique_ids[vidI]]\n",
        "    vedI_data=vedI_data.reset_index(drop=True)\n",
        "    total_num_trending=vedI_data.shape[0]\n",
        "\n",
        "    titleI = TextBlob(remove_punctuation(vedI_data.title[0])) #removing the punctuation results in changing sentiment results slightly\n",
        "    tagsI = TextBlob(remove_punctuation(vedI_data.tags[0]))\n",
        "    descriptionI = TextBlob(remove_punctuation(str(vedI_data.description[0])))\n",
        "    title_tfidf=avg_tfidf_fun(titleI,bloblist)\n",
        "    tags_tfidf = avg_tfidf_fun(tagsI, bloblist)\n",
        "    description_tfidf = avg_tfidf_fun(descriptionI, bloblist)\n",
        "\n",
        "    trending_date=pd.to_datetime(vedI_data.trending_date[0], format='%y.%d.%m')\n",
        "    published_date=pd.to_datetime(vedI_data.publishedAt[0], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    date_published_trending=trending_date -published_date\n",
        "    days_published_trending=float(date_published_trending._d)+(float(date_published_trending._h)/24)\n",
        "\n",
        "    for dayI in range(total_num_trending):\n",
        "        #print('day number :' + str(dayI))\n",
        "        df_label.video_ind[count1] = vidI\n",
        "        df_label.days_will_be_trending[count1]=total_num_trending-dayI-1\n",
        "\n",
        "        df_features.days_was_trending[count1]=dayI+1\n",
        "\n",
        "        df_features.title_polarity[count1] =titleI.sentiment.polarity\n",
        "        df_features.title_subjectivity[count1] = titleI.sentiment.subjectivity\n",
        "        df_features.title_tfidf[count1]=title_tfidf\n",
        "\n",
        "        df_features.tags_polarity[count1] =tagsI.sentiment.polarity\n",
        "        df_features.tags_subjectivity[count1] = tagsI.sentiment.subjectivity\n",
        "        df_features.tags_tfidf[count1]=tags_tfidf\n",
        "\n",
        "        df_features.description_polarity[count1] =descriptionI.sentiment.polarity #average for all the sentences\n",
        "        df_features.description_subjectivity[count1] = descriptionI.sentiment.subjectivity\n",
        "        df_features.description_tfidf[count1]=description_tfidf\n",
        "\n",
        "        df_features.categoryId[count1]=vedI_data.categoryId[dayI]\n",
        "        df_features.days_since_uploaded[count1]=days_published_trending+dayI+1\n",
        "        df_features.view_count[count1]=vedI_data.view_count[dayI]\n",
        "        df_features.likes[count1]=vedI_data.likes[dayI]\n",
        "        df_features.dislikes[count1]=vedI_data.dislikes[dayI]\n",
        "        df_features.comment_count[count1]=vedI_data.comment_count[dayI]\n",
        "\n",
        "        df_features.like_rate[count1]=vedI_data.likes[dayI]/vedI_data.view_count[dayI]\n",
        "        df_features.dislike_rate[count1]=vedI_data.dislikes[dayI]/vedI_data.view_count[dayI]\n",
        "        df_features.comment_rate[count1]=vedI_data.comment_count[dayI]/vedI_data.view_count[dayI]\n",
        "\n",
        "        if dayI==0: #no change for day 1 but give it 1 for each feature ;)\n",
        "            df_features.view_change[count1]=1\n",
        "            df_features.like_change[count1]=1\n",
        "            df_features.dislike_change[count1]=1\n",
        "            df_features.comment_change[count1]=1\n",
        "        else:\n",
        "            df_features.view_change[count1]=vedI_data.view_count[dayI]-vedI_data.view_count[dayI-1]\n",
        "            df_features.like_change[count1]=vedI_data.likes[dayI]-vedI_data.likes[dayI-1]\n",
        "            df_features.dislike_change[count1]=vedI_data.dislikes[dayI]-vedI_data.dislikes[dayI-1]\n",
        "            df_features.comment_change[count1]=vedI_data.comment_count[dayI]-vedI_data.comment_count[dayI-1]\n",
        "\n",
        "        df_features.comments_disabled[count1] = np.double(vedI_data.comments_disabled[dayI])\n",
        "        df_features.ratings_disabled[count1] = np.double(vedI_data.ratings_disabled[dayI])\n",
        "        count1+=1\n",
        "\n",
        "with open('/NLP_data.pkl', 'wb') as Data:\n",
        "    pickle.dump([df_features, df_label, numFeatures, bloblist, unique_ids], Data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting the features of video #:0\n",
            "Extracting the features of video #:1\n",
            "Extracting the features of video #:2\n",
            "Extracting the features of video #:3\n",
            "Extracting the features of video #:4\n",
            "Extracting the features of video #:5\n",
            "Extracting the features of video #:6\n",
            "Extracting the features of video #:7\n",
            "Extracting the features of video #:8\n",
            "Extracting the features of video #:9\n",
            "Extracting the features of video #:10\n",
            "Extracting the features of video #:11\n",
            "Extracting the features of video #:12\n",
            "Extracting the features of video #:13\n",
            "Extracting the features of video #:14\n",
            "Extracting the features of video #:15\n",
            "Extracting the features of video #:16\n",
            "Extracting the features of video #:17\n",
            "Extracting the features of video #:18\n",
            "Extracting the features of video #:19\n",
            "Extracting the features of video #:20\n",
            "Extracting the features of video #:21\n",
            "Extracting the features of video #:22\n",
            "Extracting the features of video #:23\n",
            "Extracting the features of video #:24\n",
            "Extracting the features of video #:25\n",
            "Extracting the features of video #:26\n",
            "Extracting the features of video #:27\n",
            "Extracting the features of video #:28\n",
            "Extracting the features of video #:29\n",
            "Extracting the features of video #:30\n",
            "Extracting the features of video #:31\n",
            "Extracting the features of video #:32\n",
            "Extracting the features of video #:33\n",
            "Extracting the features of video #:34\n",
            "Extracting the features of video #:35\n",
            "Extracting the features of video #:36\n",
            "Extracting the features of video #:37\n",
            "Extracting the features of video #:38\n",
            "Extracting the features of video #:39\n",
            "Extracting the features of video #:40\n",
            "Extracting the features of video #:41\n",
            "Extracting the features of video #:42\n",
            "Extracting the features of video #:43\n",
            "Extracting the features of video #:44\n",
            "Extracting the features of video #:45\n",
            "Extracting the features of video #:46\n",
            "Extracting the features of video #:47\n",
            "Extracting the features of video #:48\n",
            "Extracting the features of video #:49\n",
            "Extracting the features of video #:50\n",
            "Extracting the features of video #:51\n",
            "Extracting the features of video #:52\n",
            "Extracting the features of video #:53\n",
            "Extracting the features of video #:54\n",
            "Extracting the features of video #:55\n",
            "Extracting the features of video #:56\n",
            "Extracting the features of video #:57\n",
            "Extracting the features of video #:58\n",
            "Extracting the features of video #:59\n",
            "Extracting the features of video #:60\n",
            "Extracting the features of video #:61\n",
            "Extracting the features of video #:62\n",
            "Extracting the features of video #:63\n",
            "Extracting the features of video #:64\n",
            "Extracting the features of video #:65\n",
            "Extracting the features of video #:66\n",
            "Extracting the features of video #:67\n",
            "Extracting the features of video #:68\n",
            "Extracting the features of video #:69\n",
            "Extracting the features of video #:70\n",
            "Extracting the features of video #:71\n",
            "Extracting the features of video #:72\n",
            "Extracting the features of video #:73\n",
            "Extracting the features of video #:74\n",
            "Extracting the features of video #:75\n",
            "Extracting the features of video #:76\n",
            "Extracting the features of video #:77\n",
            "Extracting the features of video #:78\n",
            "Extracting the features of video #:79\n",
            "Extracting the features of video #:80\n",
            "Extracting the features of video #:81\n",
            "Extracting the features of video #:82\n",
            "Extracting the features of video #:83\n",
            "Extracting the features of video #:84\n",
            "Extracting the features of video #:85\n",
            "Extracting the features of video #:86\n",
            "Extracting the features of video #:87\n",
            "Extracting the features of video #:88\n",
            "Extracting the features of video #:89\n",
            "Extracting the features of video #:90\n",
            "Extracting the features of video #:91\n",
            "Extracting the features of video #:92\n",
            "Extracting the features of video #:93\n",
            "Extracting the features of video #:94\n",
            "Extracting the features of video #:95\n",
            "Extracting the features of video #:96\n",
            "Extracting the features of video #:97\n",
            "Extracting the features of video #:98\n",
            "Extracting the features of video #:99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX5W-fWcMAr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "c3651ee2-dc26-4906-f996-eebddb027097"
      },
      "source": [
        "df_features"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>days_was_trending</th>\n",
              "      <th>title_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_tfidf</th>\n",
              "      <th>tags_polarity</th>\n",
              "      <th>tags_subjectivity</th>\n",
              "      <th>tags_tfidf</th>\n",
              "      <th>description_polarity</th>\n",
              "      <th>description_subjectivity</th>\n",
              "      <th>description_tfidf</th>\n",
              "      <th>categoryId</th>\n",
              "      <th>days_since_uploaded</th>\n",
              "      <th>view_count</th>\n",
              "      <th>likes</th>\n",
              "      <th>dislikes</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>like_rate</th>\n",
              "      <th>dislike_rate</th>\n",
              "      <th>comment_rate</th>\n",
              "      <th>view_change</th>\n",
              "      <th>like_change</th>\n",
              "      <th>dislike_change</th>\n",
              "      <th>comment_change</th>\n",
              "      <th>comments_disabled</th>\n",
              "      <th>ratings_disabled</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6103246.0</td>\n",
              "      <td>408534.0</td>\n",
              "      <td>4498.0</td>\n",
              "      <td>27080.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10405554.0</td>\n",
              "      <td>575991.0</td>\n",
              "      <td>7357.0</td>\n",
              "      <td>35331.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4302308.0</td>\n",
              "      <td>167457.0</td>\n",
              "      <td>2859.0</td>\n",
              "      <td>8251.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13437136.0</td>\n",
              "      <td>654742.0</td>\n",
              "      <td>9338.0</td>\n",
              "      <td>39023.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3031582.0</td>\n",
              "      <td>78751.0</td>\n",
              "      <td>1981.0</td>\n",
              "      <td>3692.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16263223.0</td>\n",
              "      <td>738446.0</td>\n",
              "      <td>11755.0</td>\n",
              "      <td>43520.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2826087.0</td>\n",
              "      <td>83704.0</td>\n",
              "      <td>2417.0</td>\n",
              "      <td>4497.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>23042934.0</td>\n",
              "      <td>933351.0</td>\n",
              "      <td>19470.0</td>\n",
              "      <td>55345.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6779711.0</td>\n",
              "      <td>194905.0</td>\n",
              "      <td>7715.0</td>\n",
              "      <td>11825.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3402</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3403</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3405</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3406</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3407 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      days_was_trending  title_polarity  ...  comments_disabled  ratings_disabled\n",
              "0                   1.0             0.0  ...                0.0               0.0\n",
              "1                   2.0             0.0  ...                0.0               0.0\n",
              "2                   3.0             0.0  ...                0.0               0.0\n",
              "3                   4.0             0.0  ...                0.0               0.0\n",
              "4                   5.0             0.0  ...                0.0               0.0\n",
              "...                 ...             ...  ...                ...               ...\n",
              "3402                0.0             0.0  ...                0.0               0.0\n",
              "3403                0.0             0.0  ...                0.0               0.0\n",
              "3404                0.0             0.0  ...                0.0               0.0\n",
              "3405                0.0             0.0  ...                0.0               0.0\n",
              "3406                0.0             0.0  ...                0.0               0.0\n",
              "\n",
              "[3407 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNAk537TMrmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWYkVIfPMDyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e82e39a0-05a0-4dfd-c188-f2fd8a5653b4"
      },
      "source": [
        "df_label"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_ind</th>\n",
              "      <th>days_will_be_trending</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3402</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3403</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3405</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3406</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3407 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      video_ind  days_will_be_trending\n",
              "0           0.0                    9.0\n",
              "1           0.0                    8.0\n",
              "2           0.0                    7.0\n",
              "3           0.0                    6.0\n",
              "4           0.0                    5.0\n",
              "...         ...                    ...\n",
              "3402        0.0                    0.0\n",
              "3403        0.0                    0.0\n",
              "3404        0.0                    0.0\n",
              "3405        0.0                    0.0\n",
              "3406        0.0                    0.0\n",
              "\n",
              "[3407 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNWAR1ND8ZaL",
        "colab_type": "code",
        "outputId": "f5d84cf8-3445-42b1-fdd4-445bac90d5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np, pickle, os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import plot_importance"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyArxMEtOgKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6f866490-5a98-4b38-9485-c3bce99f4a2e"
      },
      "source": [
        "# reformat the data\n",
        "list_videos = np.array(df_label['video_ind'])  # contains indeces for all the videos with repetition.\n",
        "print(\"list videos \", list_videos)\n",
        "num_videos = np.int32(list_videos[-1])\n",
        "print(\"num videos \", num_videos)\n",
        "list_features = np.empty((num_videos,), dtype=object)\n",
        "print(\"list features \", list_features)\n",
        "list_label = np.empty((num_videos,), dtype=object)\n",
        "print(\"list label \", list_label)\n",
        "for i in range(num_videos):\n",
        "    list_features[i] = df_features[df_label['video_ind'] == i]\n",
        "    print(\"list_features[i] \", list_features[i])\n",
        "    list_label[i] = df_label.loc[df_label['video_ind'] == i, 'days_will_be_trending']\n",
        "    print(\"list_label[i] \", list_label[i])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "list videos  [0. 0. 0. ... 0. 0. 0.]\n",
            "num videos  0\n",
            "list features  []\n",
            "list label  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yODYFJcnQFWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the data into (60% training + 20% validation), 20%testing\n",
        "list_features_train, list_features_test, list_label_train, list_label_test = train_test_split(df_features,\n",
        "                                                                                              df_label,\n",
        "                                                                                              test_size=0.2,\n",
        "                                                                                              random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rPIanAcQuMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_features_train, list_features_val, list_label_train, list_label_val = train_test_split(list_features_train,\n",
        "                                                                                                    list_label_train,\n",
        "                                                                                                    test_size=0.2,\n",
        "                                                                                                    random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb92OdMxQzP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_videos_train = list_features_train.shape[0]  # number of videos for training\n",
        "num_videos_val = list_features_val.shape[0]  # number of videos for validation\n",
        "num_videos_test = list_features_test.shape[0]  # number of videos for testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCe2F-U7Q3Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "375f8cef-ef62-490f-a058-57c9a27c1c49"
      },
      "source": [
        "num_videos_train"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBdvaq1Q5IK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "368ee3bf-922e-4bff-df63-e2b96e96f08c"
      },
      "source": [
        "num_videos_test"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "682"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBWkc5myQ7R_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8973f25b-6e30-4fb7-c60e-0ae645687a52"
      },
      "source": [
        "num_videos_val"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Z-lVeLAcI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Plotting functions and others #############################################################\n",
        "def plot_fun(loss_train, loss_val, accuracy_train_All, accuracy_val_All):\n",
        "    plt.cla()\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(np.arange(1, loss_train.__len__() + 1), loss_train, 'b', np.arange(1, loss_val.__len__() + 1),\n",
        "             loss_val, 'g')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.legend(['Training Loss', 'Validation Loss'])\n",
        "    # plt.ylim([0, 3])\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(np.arange(1, accuracy_train_All.__len__() + 1), accuracy_train_All, 'b-s',\n",
        "             np.arange(1, accuracy_train_All.__len__() + 1),\n",
        "             accuracy_val_All, 'g-^')\n",
        "    plt.ylabel('corrcoef')\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.legend(['Training corrcoef', 'Validation corrcoef'])\n",
        "    # plt.ylim([0,100])\n",
        "    plt.draw()\n",
        "    plt.pause(0.000000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KcU-NwzAhqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_dis(x):\n",
        "    num_bins = range(int(min(x)),int(max(x)+2))\n",
        "    # the histogram of the data\n",
        "    n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='blue', alpha=0.5)\n",
        "\n",
        "    # add a 'best fit' line\n",
        "    #y = mlab.normpdf(bins, np.mean(x), np.std(x))\n",
        "    #plt.plot(bins, y, 'r--')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(r'Training Data Distribution')\n",
        "\n",
        "    # Tweak spacing to prevent clipping of ylabel\n",
        "    plt.subplots_adjust(left=0.15)\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_imp(model):\n",
        "    plot_importance(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ub1o8HXA8LT",
        "colab_type": "code",
        "outputId": "32f79b0c-0a45-42bb-f952-a87ccb63df60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "################################### XGBoost hyperparameters #################################################\n",
        "\n",
        "SaveDirParent = '/'\n",
        "num_classes = 1\n",
        "learning_rateI=0.1  #step size shrinkage used to prevent overfitting. Range is [0,1]\n",
        "n_estimatorsRange=[30]#range(10,100,20) #[10,30,50,..]  number of trees you want to build.\n",
        "max_depthRange=[5]#range(3,10,2) # determines how deeply each tree is allowed to grow during any boosting round.\n",
        "colsample_byteeRange=[0.3]#[i/10.0 for i in range(1,5)] #percentage of features used per tree. High value can lead to overfitting.\n",
        "#subsampleI=0.8 #percentage of samples used per tree. Low value can lead to underfitting.\n",
        "gamaI=[0.3]#[i/10.0 for i in range(0,6)] #controls whether a given node will split based on the expected reduction\n",
        "                                        # in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n",
        "#reg_alphaRange=[1e-5, 1e-2, 0.1, 1, 100] #alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n",
        "\n",
        "#Example https://www.datacamp.com/community/tutorials/xgboost-in-python#what\n",
        "#Tunning https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
        "\n",
        "\n",
        "num_classes = 1\n",
        "# truncated_backprop_length = ? # not used\n",
        "RandomStart = 1 / 4  # pick a random start for each signal between 0 and (RandomStart*signal length)\n",
        "\n",
        "################################ Import the dataset #########################################\n",
        "exists = os.path.isfile('/NLP_data_reformed.pkl')\n",
        "if exists:\n",
        "    with open('/NLP_data_reformed.pkl', 'rb') as Data:\n",
        "        [array_features_train, array_features_val, array_features_test, array_label_train,\n",
        "         array_label_val,\n",
        "         array_label_test, num_samples_train, num_samples_val, num_samples_test, features_mean, features_std,\n",
        "         numFeatures, bloblist, unique_ids]= pickle.load(Data)\n",
        "else: # Load and reformat the split data data\n",
        "    exists = os.path.isfile('/NLP_data_splits.pkl')\n",
        "    if exists:\n",
        "        with open('/NLP_data_splits.pkl', 'rb') as Data:\n",
        "            [list_features_train, list_features_val, list_features_test, list_features_train, list_label_train,\n",
        "             list_label_val,\n",
        "             list_label_test, num_videos_train, num_videos_val, num_videos_test, features_mean, features_std,\n",
        "             numFeatures,\n",
        "             bloblist, unique_ids] = pickle.load(Data)\n",
        "    else:  # Load and split the original data\n",
        "        with open('/NLP_data.pkl', 'rb') as Data:\n",
        "            [df_features, df_label, numFeatures, bloblist, unique_ids] = pickle.load(Data)\n",
        "\n",
        "        # reformat the data\n",
        "        list_videos = np.array(df_label['video_ind'])  # contains indeces for all the videos with repetition.\n",
        "        num_videos = np.int32(list_videos[-1])\n",
        "        list_features = np.empty((num_videos,), dtype=object)\n",
        "        list_label = np.empty((num_videos,), dtype=object)\n",
        "        for i in range(num_videos):\n",
        "            list_features[i] = df_features[df_label['video_ind'] == i]\n",
        "            list_label[i] = df_label.loc[df_label['video_ind'] == i, 'days_will_be_trending']\n",
        "\n",
        "        # Splitting the data into (60% training + 20% validation), 20%testing\n",
        "        list_features_train, list_features_test, list_label_train, list_label_test = train_test_split(list_features,\n",
        "                                                                                                      list_label,\n",
        "                                                                                                      test_size=0.2,\n",
        "                                                                                                      random_state=1)\n",
        "\n",
        "        list_features_train, list_features_val, list_label_train, list_label_val = train_test_split(list_features_train,\n",
        "                                                                                                    list_label_train,\n",
        "                                                                                                    test_size=0.2,\n",
        "                                                                                                    random_state=1)\n",
        "\n",
        "        num_videos_train = list_features_train.shape[0]  # number of videos for training\n",
        "        num_videos_val = list_features_val.shape[0]  # number of videos for validation\n",
        "        num_videos_test = list_features_val.shape[0]  # number of videos for testing\n",
        "\n",
        "        # normalizing the features based on mean and std of training data\n",
        "        num_samples = 0\n",
        "        features_mean = np.zeros(numFeatures)\n",
        "        features_std = np.zeros(numFeatures)\n",
        "        for video_i in range(num_videos_train):\n",
        "            features_mean = np.sum(list_features_train[video_i], axis=0) + features_mean\n",
        "            features_std = np.std(list_features_train[video_i], axis=0) + features_std\n",
        "            num_samples = num_samples + list_features_train[video_i].shape[0]\n",
        "\n",
        "        features_mean = features_mean / num_samples\n",
        "        features_std = features_std / num_samples\n",
        "\n",
        "        # normalize training data\n",
        "        for video_i in range(num_videos_train):\n",
        "            list_features_train[video_i] = (list_features_train[video_i] - features_mean) / features_std\n",
        "\n",
        "        # normalize validation data\n",
        "        for video_i in range(num_videos_val):\n",
        "            list_features_val[video_i] = (list_features_val[video_i] - features_mean) / features_std\n",
        "        # normalize testing data\n",
        "        for video_i in range(num_videos_test):\n",
        "            list_features_test[video_i] = (list_features_test[video_i] - features_mean) / features_std\n",
        "\n",
        "        with open('/NLP_data_splits.pkl', 'wb') as Data:\n",
        "            pickle.dump(\n",
        "                [list_features_train, list_features_val, list_features_test, list_features_train, list_label_train,\n",
        "                 list_label_val,\n",
        "                 list_label_test, num_videos_train, num_videos_val, num_videos_test, features_mean, features_std,\n",
        "                 numFeatures,\n",
        "                 bloblist, unique_ids], Data)\n",
        "\n",
        "    # reformat the data to one array for training , validation testing\n",
        "    num_samples_train = 0\n",
        "    for video_i in range(num_videos_train):\n",
        "        num_samples_train = num_samples_train + list_features_train[video_i].shape[0]\n",
        "    num_samples_val = 0\n",
        "    for video_i in range(num_videos_val):\n",
        "        num_samples_val = num_samples_val + list_features_val[video_i].shape[0]\n",
        "    num_samples_test = 0\n",
        "    for video_i in range(num_videos_test):\n",
        "        num_samples_test = num_samples_test + list_features_test[video_i].shape[0]\n",
        "\n",
        "    array_features_train = np.zeros([num_samples_train, numFeatures])\n",
        "    array_label_train = np.zeros([num_samples_train])\n",
        "    countI = 0\n",
        "    for video_i in range(num_videos_train):\n",
        "        indFrom = countI\n",
        "        indTo = indFrom + list_features_train[video_i].shape[0]\n",
        "        array_features_train[indFrom:indTo, :] = np.array(list_features_train[video_i])\n",
        "        array_label_train[indFrom:indTo] = np.array(list_label_train[video_i])\n",
        "        countI = indTo\n",
        "\n",
        "    array_features_val = np.zeros([num_samples_val, numFeatures])\n",
        "    array_label_val = np.zeros([num_samples_val])\n",
        "    countI = 0\n",
        "    for video_i in range(num_videos_val):\n",
        "        indFrom = countI\n",
        "        indTo = indFrom + list_features_val[video_i].shape[0]\n",
        "        array_features_val[indFrom:indTo, :] = np.array(list_features_val[video_i])\n",
        "        array_label_val[indFrom:indTo] = np.array(list_label_val[video_i])\n",
        "        countI = indTo\n",
        "\n",
        "    array_features_test = np.zeros([num_samples_test, numFeatures])\n",
        "    array_label_test = np.zeros([num_samples_test])\n",
        "    countI = 0\n",
        "    for video_i in range(num_videos_test):\n",
        "        indFrom = countI\n",
        "        indTo = indFrom + list_features_test[video_i].shape[0]\n",
        "        array_features_test[indFrom:indTo, :] = np.array(list_features_test[video_i])\n",
        "        array_label_test[indFrom:indTo] = np.array(list_label_test[video_i])\n",
        "        countI = indTo\n",
        "\n",
        "    with open('/NLP_data_reformed.pkl', 'wb') as Data:\n",
        "        pickle.dump(\n",
        "            [array_features_train, array_features_val, array_features_test, array_label_train,\n",
        "             array_label_val,\n",
        "             array_label_test, num_samples_train, num_samples_val, num_samples_test, features_mean, features_std,\n",
        "             numFeatures, bloblist, unique_ids], Data)\n",
        "\n",
        "#Random oversampling\n",
        "ros = SMOTE(random_state=42,k_neighbors=3)\n",
        "array_features_train, array_label_train = ros.fit_resample(array_features_train, array_label_train)\n",
        "\n",
        "\n",
        "# Saving results parameters\n",
        "savingTime = 1  # save after N epochs\n",
        "Results_train_All = np.zeros((len(n_estimatorsRange) * len(max_depthRange) * len(colsample_byteeRange),\n",
        "                              5))  # saving the results for each fold in CV, for 160 trials and for 5 metrics\n",
        "Results_val_All = np.zeros((len(n_estimatorsRange) * len(max_depthRange) * len(colsample_byteeRange), 5))  #\n",
        "CVpredictionsMaxCorr = np.zeros((num_samples_test))\n",
        "CVpredictionsMinRMSE = np.zeros((num_samples_test))\n",
        "countSave = 0\n",
        "highestInnCorr = -1  # initial values\n",
        "lowestInnRMSE = 100\n",
        "\n",
        "for n_estimatorsI in  n_estimatorsRange:\n",
        "    for max_depthI in max_depthRange:\n",
        "        for colsample_byteeI in colsample_byteeRange:\n",
        "            xg_reg = xgb.XGBRegressor(objective='reg:linear', colsample_bytree=colsample_byteeI,\n",
        "                                      learning_rate=learning_rateI,\n",
        "                                      max_depth=max_depthI, gama=gamaI, n_estimators=n_estimatorsI)\n",
        "            xg_reg.fit(array_features_train, array_label_train)\n",
        "\n",
        "            # Results on training data\n",
        "            predictionsPerDay = xg_reg.predict(array_features_train)\n",
        "\n",
        "            Results_train_All[countSave, 0] = np.sqrt(mean_squared_error(array_label_train, predictionsPerDay))\n",
        "            Results_train_All[countSave, 1] = mean_absolute_error(array_label_train, predictionsPerDay)\n",
        "            Results_train_All[countSave, 2] = r2_score(array_label_train, predictionsPerDay)\n",
        "            corrI = stats.pearsonr(array_label_train, predictionsPerDay)\n",
        "            Results_train_All[countSave, 3] = corrI[0]\n",
        "            Results_train_All[countSave, 4] = corrI[1]\n",
        "\n",
        "            # Results on validation data\n",
        "            predictionsPerDay = xg_reg.predict(array_features_val)\n",
        "\n",
        "            Results_val_All[countSave, 0] = np.sqrt(mean_squared_error(array_label_val, predictionsPerDay))\n",
        "            Results_val_All[countSave, 1] = mean_absolute_error(array_label_val, predictionsPerDay)\n",
        "            Results_val_All[countSave, 2] = r2_score(array_label_val, predictionsPerDay)\n",
        "            corrI = stats.pearsonr(array_label_val, predictionsPerDay)\n",
        "            Results_val_All[countSave, 3] = corrI[0]\n",
        "            Results_val_All[countSave, 4] = corrI[1]\n",
        "            print(\"XGboost with estimators # %d, max depth # %d, training RMSE %.2f, validation RMSE %.2f\" % (\n",
        "            n_estimatorsI, max_depthI,\n",
        "            Results_train_All[\n",
        "                countSave, 0],\n",
        "            Results_val_All[\n",
        "                countSave, 0]))\n",
        "\n",
        "            # Saving the model with the highest validation correlation\n",
        "\n",
        "            if (Results_val_All[countSave, 3] > highestInnCorr and Results_val_All[countSave, 3] ==\n",
        "                    Results_val_All[countSave, 3]):\n",
        "                save_path = SaveDirParent + 'xgbMode' + '_CorrCV.dat'\n",
        "                pickle.dump(xg_reg, open(save_path, \"wb\"))\n",
        "                highestInnCorr = Results_val_All[countSave, 3]\n",
        "                print(\"Model saved in file: %s\" % save_path)\n",
        "            if (Results_val_All[countSave, 0] < lowestInnRMSE and Results_val_All[countSave, 0] ==\n",
        "                    Results_val_All[countSave, 0]):\n",
        "                save_path = SaveDirParent + 'xgbMode' + '_RMSECV.dat'\n",
        "                pickle.dump(xg_reg, open(save_path, \"wb\"))\n",
        "                lowestInnRMSE = Results_val_All[countSave, 0]\n",
        "                print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "            countSave = countSave + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Results on Testing data\n",
        "# Using saved best model with highest correlation to find testing predictions and save it\n",
        "save_path = SaveDirParent +'xgbMode'+ '_CorrCV.dat'\n",
        "xg_reg = pickle.load(open(save_path, \"rb\"))\n",
        "predictions_MaxCorr= xg_reg.predict(array_features_test)\n",
        "\n",
        "# Using saved best model with lowest RMSE to find testing predictions and save it\n",
        "save_path = SaveDirParent + 'xgbMode' + '_RMSECV.dat'\n",
        "xg_reg = pickle.load(open(save_path, \"rb\"))\n",
        "predictions_MinRMSE = xg_reg.predict(array_features_test)\n",
        "\n",
        "# Finding Final Testing Results\n",
        "corrI = stats.pearsonr(array_label_test, predictions_MaxCorr)\n",
        "class_reportMaxCorr = [np.sqrt(mean_squared_error(array_label_test, predictions_MaxCorr)),\n",
        "                       mean_absolute_error(array_label_test, predictions_MaxCorr),\n",
        "                       r2_score(array_label_test, predictions_MaxCorr), corrI[0], corrI[1]]\n",
        "print(\n",
        "    \"Final testing using Max validation Corr: RMSE %.2f, MAE %.2f, R2 score %.2f, Correlation coefficient %.2f (p=%.4f).\" % (\n",
        "        np.sqrt(mean_squared_error(array_label_test, predictions_MaxCorr)),\n",
        "        mean_absolute_error(array_label_test, predictions_MaxCorr),\n",
        "        r2_score(array_label_test, predictions_MaxCorr),\n",
        "        corrI[0], corrI[1]))\n",
        "corrI = stats.pearsonr(array_label_test, predictions_MinRMSE)\n",
        "class_reportMinRMSE = [np.sqrt(mean_squared_error(array_label_test, predictions_MinRMSE)),\n",
        "                       mean_absolute_error(array_label_test, predictions_MinRMSE),\n",
        "                       r2_score(array_label_test, predictions_MinRMSE), corrI[0], corrI[1]]\n",
        "\n",
        "print(\n",
        "    \"Final testing using Min validation RMSE: RMSE %.2f, MAE %.2f, R2 score %.2f, Correlation coefficient %.2f (p=%.4f).\" % (\n",
        "        np.sqrt(mean_squared_error(array_label_test, predictions_MinRMSE)),\n",
        "        mean_absolute_error(array_label_test, predictions_MinRMSE),\n",
        "        r2_score(array_label_test, predictions_MinRMSE),\n",
        "        corrI[0], corrI[1]))\n",
        "\n",
        "###############Saving the variables#################\n",
        "# Saving the objects:\n",
        "with open(SaveDirParent + 'XGboostResults.pkl', 'wb') as Results:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump(\n",
        "        [Results_train_All, Results_val_All, class_reportMaxCorr, class_reportMinRMSE,\n",
        "         predictions_MinRMSE, array_label_test, predictions_MaxCorr], Results)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5282ccf874ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                                                                                                       \u001b[0mlist_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                                                                                       \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                                                                                       random_state=1)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         list_features_train, list_features_val, list_label_train, list_label_val = train_test_split(list_features_train,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2122\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1805\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8L9KHUnNlse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a50253b5-78ef-4065-debb-ed8b542146e5"
      },
      "source": [
        "list_features"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDV0o-eN3_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "93db4281-0af7-43a0-b236-a5255d1ef632"
      },
      "source": [
        "list_features_train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-63b62c9d60c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_features_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'list_features_train' is not defined"
          ]
        }
      ]
    }
  ]
}